# ðŸ§ª Brewery ETL Pipeline with Airflow

This repository contains a complete ELT pipeline for processing data from the Open Brewery DB API using Apache Airflow. It follows the **Medallion architecture** (Raw â†’ Silver â†’ Gold) and is orchestrated through Airflow running in Docker.

---

## ðŸš€ Project Overview

This ETL process performs:

1. **Extraction** of brewery data from a public API
2. **Transformation** into a clean silver dataset
3. **Aggregation** of metrics by location for gold-level analysis
4. **Persistence** of all data in JSON and Parquet formats under a local `/data` folder, mounted to the Airflow container

The pipeline is containerized using Docker and all common operations are automated via a `Makefile`.

---

## âš™ï¸ Environment Setup

> Ensure [Docker Desktop](https://www.docker.com/products/docker-desktop/) is installed and running before continuing.

### ðŸ§° Main Setup Commands

All setup commands are available in the `Makefile`. Run them from your terminal in the project root:

```bash
make permissions
```

- Creates the necessary folders: dags/, logs/, plugins/, config/, and data/

- Sets permissions if needed


```bash
make up
```

- Downloads and runs the official Apache Airflow docker-compose.yaml

- Ensures the data/ volume is mounted via sed

- Starts all required services (scheduler, webserver, etc.)


## Airflow Access
**URL**: http://localhost:8080

**Username**: airflow

**Password**: airflow

## Volumes & Persistence
These folders on your host are mapped to the Airflow container:

```yaml
volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
```

- Changes to these directories in the container will reflect on your local machine and vice versa.

## Virtual Environment (for development)
Used to run unit tests and format Python files locally.

```bash
make virtual_env
source .venv/bin/activate
make requirements
```

- **make virtual_env**: Creates a virtual environment in .venv/

- **source .venv/bin/activate**: Activates the environment

- **make requirements**: Installs project dependencies from requirements.txt


## Code Quality & Formatting
Before pushing code, ensure it follows consistent formatting:

```bash
make format
```
Runs:

- **black**: Auto-formatter

- **isort**: Organizes import statements

```bash
make unit_tests
```

Runs unit tests in tests/test_functions.py

```bash
make check
```

Used in CI to validate formatting and test execution.


## Unit Testing & Coverage
To check your tests with coverage:

```bash
coverage run -m unittest discover tests/
coverage report -m
```

## Shutting Down
To stop the Airflow containers when you're done:

```bash
make down
```

## Project Structure

```graphql

â”œâ”€â”€ .github/
    â”œâ”€â”€ workflows
        â””â”€â”€ unit-test.yml # CI workflow
â”œâ”€â”€ config/               # Configs if needed
â”œâ”€â”€ dags/                 # All DAG definitions and ETL scripts
â”‚   â”œâ”€â”€ src/              # Python modules for extract, transform, load
â”‚   â””â”€â”€ brewery_dag.py    # Main Airflow DAG
â”œâ”€â”€ data/                 # Data folder (mounted inside the container)
â”‚   â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ silver_data/
â”‚   â””â”€â”€ gold_data/
â”œâ”€â”€ logs/                 # Airflow logs
â”œâ”€â”€ plugins/              # Custom Airflow plugins (if any)
â”œâ”€â”€ tests/                # Unit test folder
â”‚   â””â”€â”€ test_functions.py
â”œâ”€â”€ .env                  # Airflow ENV
â”œâ”€â”€ .gitignore            # Declare what folders/files will be ignored
â”œâ”€â”€ docker-compose.yaml  # Airflow infrastructure
â”œâ”€â”€ Makefile              # Automates all key commands
â””â”€â”€ README.md             # This file
â”œâ”€â”€ requirements.txt      # Python dependency file

```

## Additional Notes
- **Timezone**: DAG schedules use UTC. Daily runs are configured at 23:00 UTC (8:00 PM Brazil time).

- **Retries & Resilience**: Python scripts include try/except handling. Airflow DAGs can be configured with retries if needed (see retries, retry_delay).

- **Parametrized DAG**: DAG parameters are passed via params={} and injected into BashOperator scripts via Jinja ({{ params.xyz }}). The required parameters are:
    - **raw_final_path**  
    - **raw_file_name**
    - **url_api**
    - **silver_path**
    - **gold_path**

- **Monitoring / Logging**: All Python scripts include structured logging for observability in Airflow logs. Messages that start with "Observability" can be mapped and then integrated with a visualazation tool like Datadog or Grafana

- **Testing**: Modular design allows each Python function to be tested in isolation.

## Repository Patterns and Data Quality
To maintain consistency and ensure code quality, this repository uses a CI pipeline to enforce naming conventions for commits and branches. See the desired commit patters:

<type>: <description>

Where <type> is one of the following:

- feat â€“ New feature
- fix â€“ Bug fix
- docs â€“ Documentation changes
- chore â€“ Maintenance tasks
- refactor â€“ Code refactoring that doesn't affect behavior
- test â€“ Adding or updating tests
- build â€“ Build system changes
- ci â€“ Continuous Integration configuration
- perf â€“ Performance improvements
- revert â€“ Revert a previous commit

**Example**: feat: add retry logic to ETL pipeline

## Branch Naming Convention
All branches for new features or improvements must begin with **feature/**. Example: feature/add-transformation-logging.

These conventions are enforced in CI using GitHub Actions, ensuring code hygiene across contributors.

## Unit tests with CI

It also checks for code formatting and unit tests, the same used with make format and make unit_tests commands.